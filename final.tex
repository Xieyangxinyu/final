\documentclass[10pt, oneside]{article}
\usepackage{geometry}             
\geometry{letterpaper, left = 21 mm, right = 21 mm, top = 30 mm, bottom = 25 mm}     


%------------------------macros and packages------------------

%%%Please put path to the downloaded macro file here.
\usepackage{macros} 
%-------------------------begin doc-------------------


\begin{document}

\thispagestyle{empty}
\title{Combinatorial Hypothesis Testing}


%-------------------------------------------------------------------------%

\maketitle
\addtocounter{footnote}{-1}\let\thefootnote\svthefootnote

\section{Introduction}

Suppose we observe an $n$-dimensional vector $\bX = (X_1,...,X_n)$. The null hypothesis $H_0$ is that $\bX \sim \cN(0, \bI)$. We denote the probability measure and expectation under $H_0$ by $\bP_0$ and $\bE_0$, respectively.

Combinatorics kicks in as we consider the alternative hypotheses, by introducing a class $\cC$: consider a class $\cC=\{S_1,\ldots,S_N\}$ of $N$ sets of indices such that $S_k \subset\{1,\ldots,n\}$ and $|S_k| = K$ for all $k=1,\ldots,N$. Under $H_1$, there
exists an $S \in \cC$ such that the distribution of $X_i$ is determined by whether $i$ is in $S$:
\begin{alt}[Detection of Means\cite{arias2008searching, addario2010combinatorial, arias2011detection}]
  \label{alt:mean}
$$
X_i \sim
\begin{cases}
  \cN(0,1), \quad & \mbox{ if }i \notin S\\
  \cN(\mu,1), & \mbox{ if }i \in S
\end{cases}
$$
where $\mu>0$ is a positive parameter and components of $\bX$ are independent. 
\end{alt}
\begin{alt}[Detection of Correlations\cite{arias2012correlation}]
  \label{alt:correlation}
  $X_i \sim \cN(0,1)$ for each $i$ and 
  $$
  \Cov(X_i, X_j) =
  \begin{cases}
    1, \quad & \mbox{ if }i = j\\
    \rho, & \mbox{ if }i \neq j\mbox{ with }i, j \in S\\
    0, & \mbox{ otherwise}\\
  \end{cases}
  $$
  where $\rho \in (0,1).$
\end{alt}

For each $S \in \cC$, we denote the probability measure and expectation by $\bP_S$ and $\bE_S$, respectively. Many interesting combinatorial examples of $\cC$ arises: subsets of size $K$, cliques, perfect matchings, spanning trees, and clusters, etc.

A~\textit{test} is a~binary-valued function $f:\bR^n \to\{
0,1\}$. If
$f(X)=0$, then the test accepts the null hypothesis $H_0$;
otherwise $H_0$ is rejected by $f$.
We measure the performance of a~test based on the \textit{minimax risk}:
\[
R_*^{\max} := \inf_{f} R^{\max}(f).
\]
where $R^{\max}(f)$ is the worst-case risk over the class of interest $\cC$, formally defined by
\[
R^{\max}(f) = \bP_0\{f(X)=1\}
+ \max_{S\in\cC} \bP_S\{f(X)=0\}.
\]

In this report, we discuss the techniques introduced in \cite{arias2012correlation, addario2010combinatorial, arias2011detection} to derive the lower and upper bounds of $R_*^{\max}$, as well as more recent extensions.

\section{General Framework}

\subsection{Lower Bounds}
A~standard way to obtain lower bounds for the minimax risk is by putting a~prior on the class $\cC$ and obtaining a~lower bound on the corresponding \textit{Bayesian risk}, which never exceeds the worst-case risk. The
uniform prior on $\cC$ gives rise to the following \textit{average risk}:
\[
R(f) = \bP_0\{f(X)=1\}
+ \bP_1\{f(X)=0\},
\]
where
\[
\bP_1\{f(X)=0\} := \frac{1}{N}\sum_{S\in\cC} \bP_S\{f(X)=0\},
\]
and $N := |\cC|$ is the cardinality of $\cC$.
The advantage of the average risk over the worst-case risk is that the likelihood ratio test, denoted $f^*$, is optimal for the former, according to the Neyman--Pearson lemma. Introducing $L(X)$, the likelihood ratio between $H_0$ and $H_1$, the optimal test becomes
%
\[
f^*(x) = 0  \quad\mbox{if and only if}\quad   L(x) \le 1.
\]
The
%GL
(average)
risk $R^*=R(f^*)$ of the optimal test is called the
\textit{Bayes risk} and it satisfies

\[
R^* = 1 - \frac{1}{2} \bE_0 |L(X) - 1|
\]

\subsection{Upper Bounds}
The analysss of the upper bounds of the risks of optimal tests are often less systematic. Even though likelihood ratio test is optimal in the
Bayesian setting, it is difficult to obtain directly upper bounds for the (worst-case) risk
of the likelihood ratio test. Hence, in analysing combinatorial testing problems, we often focus on obtaining upper bounds for sub-optimal tests for ease of analysis and strive for an upper bound matching the obtained lower bound.


\section{Detection of Means}
In this section we focus on the first alternative hypothesis \ref{alt:mean}. We will first discuss simple tests are considered as we obtain upper bounds: the averaging test and the maximum test. Then, we will provide the key insights driving the results for lower bounds obtained in \cite{addario2010combinatorial} and discuss the applications to examples such as $k$-sets, spanning trees, perfect matchings, etc.


\subsection{Upper Bounds}
The \textit{averaging test} has the form
\[
f(\mathbf{x}) = 0  \quad\mbox{if and only if}\quad {\{ \sum_{i=1}^n X_i \le \mu K/2 \}}.
\]
Since the sum of the components of $\bX$ is zero-mean normal under $\bP_0$
and has mean $\mu K$ under the alternative hypothesis, we obtain the following:
\begin{prop}[\cite{addario2010combinatorial}, Proposition 2.1]
  \label{average}
  Let $\delta>0$. The risk of the averaging test $f$ satisfies
  $R(f) \le\delta$
  whenever
  %
  \[
  \mu\ge\sqrt{\frac{8n}{K^2} \log\frac{2}{\delta}}.
  \]
  %
\end{prop}

The \textit{maximum test} is based on the \textit{scan statistic} $\max_{S\in\cC} X_S$, and it has the form:
%
\[
f(\mathbf{x}) = 1 \quad\mbox{if and only if}\quad
\max_{S\in\cC} X_S\ge\frac{\mu K + \mathbb{E}_0 \max_{S\in\cC} X_S}{2}.
\]
Observing that under the alternative hypothesis for some $S\in\cC$, $X_S= \sum_{i\in S} X_i$ is normal
$(\mu K, K)$, we obtain
\begin{prop}[\cite{addario2010combinatorial}, Proposition 2.2]
  \label{maxtest}
  The risk of the maximum test $f$ satisfies
  $R(f) \le\delta$ whenever
  %
  \[
  \mu\ge\frac{\mathbb{E}_0 \max_{S\in\cC} X_S}{K} + 2\sqrt{\frac{2}{K}
  \log
  \frac{2}{\delta}}.
  \]
  %
\end{prop}

{\bf ADD SOMETHING}

\subsection{Lower Bounds}
Let's take a closer look at the likehood ratio. If we write 
%
\[
\phi_0(\mathbf{x}) = (2\pi)^{-n/2} e^{-\sum_{i=1}^n x_i^2/2}
\]
%
and
%
\[
\phi_S(\mathbf{x}) = (2\pi)^{-n/2} e^{-\sum_{i\in S}(x_i-\mu
)^2/2-\sum
_{i\notin S} x_i^2/2}
\]
for the
probability densities of $\bP_0$ and $\bP_S$, respectively,
the likelihood ratio at $\mathbf{x}$ is
\[
L(\mathbf{x})
= \frac{{1/N}\sum_{S\in\cC} \phi_S(\mathbf{x})}{\phi
_0(\mathbf{x})}
= \frac{1}{N} \sum_{S\in\cC}
e^{\mu x_S - K\mu^2/2},
\]
where $x_S = \sum_{i\in S} x_i$. With the observation \cite{devroye2013probabilistic}
$$R^* \ge 1 - \sqrt{1 - (\mathbb{E}_0 \sqrt{L(\bX)})^2}$$
we can apply Jensen's inequality, 
\[
\mathbb{E}_0 \sqrt{L(\bX)} = \int\sqrt{\frac{{1/N}\sum_{S\in\cC} \phi_S(\mathbf{x})}{\phi
_0(\mathbf{x})}} \phi
_0(\mathbf{x}) \,d\mathbf{x}
= \int\sqrt{\frac{1}{N} \sum_{S\in\cC} \phi_S(\mathbf{x}) \phi
_0(\mathbf{x})} \,d\mathbf{x}
\ge\frac{1}{N} \sum_{S\in\cC}
\int\sqrt{ \phi_S(\mathbf{x}) \phi_0(\mathbf{x})} \,d\mathbf{x}.
\] 
Because for any $S\in\cC$,
\[
\int\sqrt{ \phi_S(\mathbf{x}) \phi_0(\mathbf{x})} \,d\mathbf
{x}=e^{-\mu^2K/8}
\]
for all classes $\cC$, $R^* \ge1/2$ whenever
\begin{equation}
  \label{eq: loose lower bound}
  \mu\le\sqrt{(4/K)}\times\sqrt{\log(4/3)}
\end{equation}
i.e. small risk cannot be achieved unless $\mu$ is substantially large compared to $K^{-1/2}$. But this can be substantially improved by the moment method proposed in \cite{addario2010combinatorial}, as we discuss next.

%
\subsubsection{Moment Methods}
\label{subsubsec:Moment Methods}
%

The moment method applies the following insight: by the Cauchy--Schwarz inequality,
%
\[
R^* = 1 - \tfrac{1}{2} \mathbb{E}_0 |L(\bX)-1|
\ge1 - \tfrac{1}{2} \sqrt{\mathbb{E}_0 |L(\bX)-1|^2}.
\]
and since $\mathbb{E}_0 L(\bX) = 1, \mathbb{E}_0 |L(\bX)-1|^2=\Var_0(L(\bX)) = \mathbb{E}_0 [L(\bX)^2] -1.$ 
We are now ready to prove the following lower bound based on overlapping pairs, which reduces the problem to
studying a purely combinatorial quantity \cite{arias2008searching,addario2010combinatorial}:
\begin{prop}[\cite{addario2010combinatorial}, Proposition 3.2]
  \label{prop:pairs}
  Let $S$ and $S'$ be drawn independently, uniformly, at random from $\cC$
  and let $Z=|S\cap S'|$. Then
  %
  \[
  R^* \ge1- \tfrac{1}{2} \sqrt{\mathbb{E} e^{\mu^2 Z} -1}.
  \]
  %
\end{prop}
\begin{proof}
Because $L(\bX)= \frac1 N \sum_{S\in\cC} e^{\mu X_S -K\mu^2/2}$, 
\[
\mathbb{E}_0 [L(\bX)^2]
=
\frac{1}{N^2} \sum_{S,S'\in\cC} e^{-K \mu^2} \mathbb{E}_0 e^{\mu(X_S+X_{S'})}.
\]
Meanwhile,
\begin{eqnarray*}
\mathbb{E}_0 e^{\mu(X_S+X_{S'})}
& = &
\mathbb{E}_0 [ e^{\mu\sum_{i\in S\setminus S'} X_i} e^{\mu\sum_{i\in
S'\setminus S} X_i}
e^{2\mu\sum_{i\in S\cap S'} X_i} ] \\
& = &
(\mathbb{E}_0 e^{\mu X} )^{2(K-|S\cap S'|)} (\mathbb{E}_0 e^{2\mu X}
)^{|S\cap S'|}
\\
& = &
e^{\mu^2 (K-|S\cap S'|)+2\mu^2|S\cap S'|}
\end{eqnarray*}
\end{proof}

The previous proposition allows us to obtain lower bounds by analysing the quantity $\mathbb{E} e^{\mu^2Z}$. 
This allows us to exploit the combinatorial structures of the class $\cC$ and gives us far better bounds than \ref{eq: loose lower bound}, as the folloiwng examples show:


\begin{exmp}[Disjoint Sets, \cite{addario2010combinatorial}, Section 4.1]
  \label{exmp: Disjoint Sets}
  Suppose all $S\in\cC$ are disjoint (and therefore $KN\le n$). Fix $\delta\in(0,1)$. Let $Z = K$ with probability $1/N$ and $Z = 0$ otherwise. Thus,
  %
  \[
  \mathbb{E} e^{\mu^2Z} -1 = \frac{1}{N} (e^{\mu^2 K} -1 )
  \le\frac{1}{N} e^{\mu^2 K}
  \]
  %
  and therefore $R^* \ge\delta$ whenever
  %
  \[
  \mu\le\sqrt{\frac{\log(4N(1-\delta)^2)}{K}}.
  \]
  Notice that with the bound the bound $\mathbb{E}_0 \max_{S\in\cC} X_S \le\sqrt{2K\log N}$, maximum test $f$ gives us $R^* \le R(f) \le\delta$
  whenever
  \[
  \mu\ge\sqrt{\frac{2\log N}{K}} + 2\sqrt{\frac{2\log(2/\delta)}{K}}.
  \]
  So in this case the critical transition occurs when $\mu$ is of the order
of $\hspace*{-0.2pt}\sqrt{(1/K)\hspace*{-0.2pt}\log\hspace*{-0.25pt} N}$.
\end{exmp}

\begin{exmp}[Spanning Trees, \cite{addario2010combinatorial}, Section 4.5]
  \label{exmp:Spanning Trees}
Let $1,2,\ldots,n={m\choose2}$ represent the edges of the complete graph $K_m$ and let $\cC$ be the set of all spanning trees of $K_m$.
Thus, we have $N=m^{m-2}$ spanning trees and $K=m-1$. With the fact $\mathbb{E}[ e^{\mu^2 Z} ] \le \exp(2e^{\mu^2 })$, we obtain that for any $\delta\in(0,1)$,
$R^* \ge\delta$ whenever
\[
\mu\le\sqrt{\log\bigl(1+\tfrac1 2 \log\bigl(1+4(1-\delta)^2\bigr) \bigr)}.
\]
Meanwhile, the averaging test has risk $R(f) \le\delta$ whenever $\mu\ge\sqrt{4\log(2/\delta)}$.
\end{exmp}

\begin{exmp}[Cliques, \cite{addario2010combinatorial}, Section 4.6]
  Consider the random variables $X_1,\ldots,X_n$ associated with the edges of the complete graph $K_m$ such that ${m\choose2}=n$ and let $\cC$ contain all cliques of size $k$. Thus, $K={k\choose2}$ and $N={m\choose k}$. With some technical work, one can show that $\mathbb{E}[ \exp(\mu^2Z) ] \le2$. This gives us $R^* \ge1/2$ whenever
%
\[
\mu\le\sqrt{\frac{1}{k} \log\biggl(\frac{m}{2k} \biggr)}.
\]
Similar to disjoint sets (Example \ref{exmp: Disjoint Sets}), we have with maximum test, $R^* \le\delta$ whenever
%
\[
\mu\ge
2\sqrt{\frac{1}{k-1} \log\biggl(\frac{me}{k} \biggr)}
+ 4\sqrt{\frac{\log(2/\delta)}{k(k-1)}};
\]
\end{exmp}

\begin{defn}
  We say that the class $\cC$ is {\it symmetric} if it satisfies the following conditions.
  Let $S,S'$ be drawn independently and uniformly at random from $\cC$. Then,
  \begin{enumerate}
    \item the conditional distribution of $Z=|S\cap S'|$ given $S'$ is identical
    for all values of $S'$;
    \item for any fixed $S_0\in\cC$ and $i\in S_0$, $\bP\{i\in S\}=K/n$.
  \end{enumerate}
\end{defn}
Via H\"older's inequality, we can obtain the folloiwng improvement of the universal lower bound obtained earlier.
\begin{prop}[\cite{addario2010combinatorial}, Proposition 3.3]
  \label{symmetric}
  Let $\delta\in(0,1)$.
  Assume that $\cC$ is symmetric. Then $R^*\ge\delta$ for all $\mu$ with
  \[
  \mu\le\sqrt{\frac{1}{K}\log\biggl(1+\frac{4n(1-\delta)^2}{K} \biggr)}.
  \]
\end{prop}
\begin{proof}
  Integrating H\"older's inequality and symmetry, we obtain 
  $$\mathbb{E}[ e^{\mu^2 Z}]\le (e^{\mu^2K} -1 ) \frac{K}{n} +1.$$
  Then we can apply Proposition \ref{prop:pairs}. We omit the details here.
\end{proof}
The proposition above shows that for any small and sufficiently symmetric
class, the critical value of $\mu$ is of the order of
$\sqrt{(\log n)/K}$, at least if $K\le n^\beta$ for some $\beta\in(0,1)$.
\begin{exmp}[Stars, \cite{addario2010combinatorial}, Section 4.4]
A star is a subgraph of the complete graph $K_m$ which contains all $K=m-1$ edges incident to a fixed vertex. Consider the set $\cC$ of all stars in $K_m$. In this setting, $n={m\choose2}$ and $N=m$. Hence, 
for any $\varepsilon>0,$ we have $\lim_{m\to\infty} R^* = 1$ if 
$$\mu\le(1-\varepsilon)\sqrt{\dfrac{\log m}{m}}$$
\end{exmp}
Another interesting property is negative association, which allow us to improve the previous lower bound further.
\begin{defn}
  A collection $Y_1,\ldots,Y_n$ of random variables is \textit{negatively associated} if for any pair of disjoint sets $I,J\subset\{1,\ldots,n\}$ and (coordinate-wise) nondecreasing functions $f$ and $g$,
\[
\mathbb{E}[ f(Y_i, i\in I) g(Y_j, j\in J) ]
\le\mathbb{E}[ f(Y_i, i\in I) ] \mathbb{E}[g(Y_j, j\in J) ].
\]
\end{defn}

\begin{prop}[\cite{addario2010combinatorial}, Proposition 3.4]
  \label{negass}
  Let $\delta\in(0,1)$ and assume that the class $\cC$ is symmetric. Suppose that the labels are such
  that $S'=\{1,2,\ldots,K\} \in\cC$. Let $S$ be a randomly chosen
  element of $\cC$.
  If the random variables
  ${\bf 1}_{\{ 1 \in S \}},\ldots,{\bf 1}_{\{ K\in S \}}$ are
  negatively associated,
  then
  $R^*\ge\delta$ for all $\mu$ with
  %
  \[
  \mu\le\sqrt{\log\biggl(1+\frac{n\log(1+4(1-\delta)^2)}{K^2} \biggr)}.
  \]
\end{prop}
\begin{proof}
  Negative association gives us
  $$\mathbb{E}[ e^{\mu^2 Z}]\le \biggl( (e^{\mu^2} -1 ) \frac{K}{n} +1 \biggr)^K.$$
  Then we can apply Proposition \ref{prop:pairs}. We omit the details here.
\end{proof}
\begin{exmp}[K-sets, \cite{addario2010combinatorial}, Section 4.2]
  Consider the example when $\cC$ contains all sets $S \subset\{1,\ldots ,n\}$ of size $K$. Note $N={n\choose K}$. This class is symmetric and satisfies the condition in the previous proposition.
\end{exmp}

\begin{exmp}[Perfect Matchings, \cite{addario2010combinatorial}, Section 4.3]
  \label{exmp:Perfect Matchings}
  Let $\cC$ be the set of all perfect matchings of the complete bipartite graph $K_{m,m}$. Thus, we have $n=m^2$ edges and $N=m!$, and $K=m$.
  The symmetry assumptions hold obviously and the negative association property follows from the fact that $Z=|S\cap S'|$ has the same
distribution as the number of fixed points in a random permutation. Hence for all $m$, $R^* \ge\delta$ whenever
\[
\mu\le\sqrt{\log\bigl(1+\log\bigl(1+4(1-\delta)^2\bigr)\bigr)}.
\]
\end{exmp}

\subsection{Detection of an Anomalous Cluster}
As a special case of the detection of means problem, we can consider the following scenario in network analysis:
The situation where no signal is present, that is, ``business as
usual,'' is modeled as
%
\[
H_0^m : X_v \sim\cN(0,1)\qquad \forall v \in\bV_m.
\]
%
Let $K$ be a cluster, which we define for now as a subset of nodes,
that is, \mbox{$K \subset\bV_m$}. In fact, we will be interested in classes of
clusters that are either derived from a geometric shape, when $\bV_m$ is
embedded in Euclidean space, or connected components, when $\bV_m$ has a
graph structure.
The situation where the nodes in $K$ behave anomalously is modeled as
%
\[
H_{1,K}^m : X_v \sim\cN(\mu_K,1)\qquad \forall v \in K;\qquad X_v \sim
\cN(0,1)\qquad \forall v \notin K,
\]
%
where $\mu_K > 0$.
%noisy data.}
We choose to decompose $\mu_K$ as $\mu_K = |K|^{-1/2} \Lambda_K$, where
$|K|$ denotes the number of nodes in $K$ and $\Lambda_K$ is the signal
strength. Indeed, with this normalization, for any cluster $K$,
%
%e1.1 ###
%
\begin{equation} \label{eq:simple-risk}
\min_T \bP(T = 1|H_0^m) + \bP(T = 0|H_{1,K}^m) = 2 \bP\bigl(\cN
(0,1) >
\Lambda_K/2\bigr),
\end{equation}
%
where the minimum is over all tests for $H_0^m$ versus $H
_{1,K}^m$ and the lower bound is achieved by the likelihood ratio
(Neyman--Pearson) test. We define
%
\[
\Lambdamax_m = \max_{K \in\Km} \Lambda_K,\qquad \Lambdamin_m =
\min_{K
\in\Km} \Lambda_K.
\]
Let $\Km$ be a class of clusters within $\bV_m$ and define
%
\[
H_1^m = \bigcup_{K \in\Km} H_{1,K}^m.
\]
%
We are interested in testing $H_0^m$ versus $H_1^m$. In other
words, under the alternative, the cluster of anomalous nodes is only
known to belong to $\Km$.
We adopt a minimax point of view.
For a test $T$, we define its worst-case risk as
%
\[
\gamma_{\Km}(T) = \bP(T = 1|H_0^m) + \max_{K \in\Km} \bP(T =
0|H_{1,K}^m).
\]
%
The minimax risk for $H_0^m$ versus $H_1^m$ is defined as
%
\[
\gamma_{\Km} = \inf_T \gamma_{\Km}(T).
\]
%
We say that $H_0^m$ and $H_1^m$ are asymptotically inseparable
(in the minimax sense) if
%
\[
\varliminf_{m \to\infty} \gamma_{\Km} = 1,
\]
%
which is equivalent to saying that, as $m$ becomes large, no test can
perform substantially better than random guessing, without even looking
at the data.
A sequence of tests $(T_m)$ is said to asymptotically separate $H
_0^m$ and $H_1^m$ if
%
\[
\lim_{m \to\infty} \gamma_{\Km}(T_m) = 0,
\]
%
and $H_0^m$ and $H_1^m$ are said to be asymptotically separable
if there is such a sequence of tests.

(under specific conditions)
$H_0^m$ and $H_1^m$ are asymptotically inseparable if there is
$\eta_m \to0$ slowly enough such that, for all $K \in\Km$,
%
\[
\Lambda_K \leq(1 - \eta_m) \sqrt{2 \log(m/|K|)};
\]
%
and conversely, we show that a version of the scan statistic
asymptotically separates $H_0^m$ and $H_1^m$ if there is $\eta_m
\to0$ slowly enough such that, for all $K \in\Km$,
%
\[
\Lambda_K \geq(1 + \eta_m) \sqrt{2 \log(m/|K|)}.
\]

\section{Detection of Correlations}
First, we note that we can rewrite the hypotheses as 
\[
H_0: \bX \sim\cN(0,\bI)\quad\mbox{vs.}\quad
H_1: \bX \sim\cN(0,\bA_S)\qquad\mbox{for some $S \in\cC$,}
\]
%
where
$\bI$ denotes the $n\times n$ identity matrix and
$$(\bA_S)_{i,j} = \begin{cases}
1, &\quad $i = j$, \cr
\rho, &\quad i \neq j, i, j \in S, \cr
0, &\quad \text{otherwise}.
\end{cases}$$
Introducing
$$Z_S = \exp\bigl(\tfrac{1}{2} X^T (\bI- \bA_S^{-1}) X \bigr)$$
for all $S \in\cC$, the likelihood ratio between $H_0$ and $H_1$ may be written as
$$L(X) = \frac{1}{N} \sum_{S \in\cC} \frac{Z_S}{\bE_0 Z_S}$$
Thus the Bayes risk satisfies
%
\[
R^* = 1 - \frac{1}{2} \bE_0 |L(X) - 1| = 1 - \frac{1}{2} \bE_0
\biggl| \frac{1}{N} \sum_{S \in\cC} \frac{Z_S}{\bE_0 Z_S} - 1\biggr|.
\]



The next representation theorem of Gaussain random variables plays a key role in analysing this test:
\begin{lem}[\cite{berman1962equally};\cite{arias2012correlation}, Lemma 1.1]
  \label{lemrepresent}
  Let $X_1,\ldots, X_k$ be standard normal with $\Cov(X_i, X_j) = \rho
  $ for $i \neq j$. Then there are i.i.d. standard normal random
  variables, denoted $U, U_1,\ldots, U_k$, such that $X_i = \sqrt{\rho}
  U + \sqrt{1-\rho}   U_i$ for all $i$.
\end{lem}


Thus, given $U$, the problem becomes that of detecting a~subset of
variables with nonzero mean (equal to $\sqrt{\rho} U$) and with a~variance equal to $1-\rho$ (instead of 1). This
simple observation will be very useful to us later on.

When $\cC$ contains just one set $S=\{1,\ldots,k\}$, we can leverage the following lemma and the fact that $\bE_0Z_S = \sqrt{\det(\bA_S)}$ to analyse the Bayes risk directly. 
\begin{lem}[\cite{arias2012correlation}, Lemma 2.1]
  \label{lemqf}
  Under $\bP_0$, $X^T (\bI- \bA_S^{-1}) X$ is distributed as
  %
  \[
  - \frac{\rho}{1-\rho} \chi^2_{k-1} + \frac{\rho(k-1)}{1 + \rho
  (k-1)} \chi^2_1,
  \]
  %
  and under the alternative $\bP_S$, it has the same distribution as
  %
  \[
  -\rho\chi^2_{k-1} + \rho(k-1) \chi^2_{1},
  \]
  %
  where $\chi_1^2$ and $\chi_{k-1}^2$ denote independent $\chi^2$
  random variables with degrees of freedom $1$ and $k-1$, respectively.
\end{lem}

\begin{prop}[\cite{arias2012correlation}, Proposition 2.1]
  $\lim_{k\to\infty}R^*= 0$ if and only if $\rho k \to\infty$. Similarly, $\lim_{k\to\infty}R^*= 1$ if and only if $\rho k \to0$.
\end{prop}
\begin{proof}
  Suppose $\rho k \to\infty$.
  It suffices to show that there exists a~threshold $\tau_k$ such that
  $\bP_0\{X^T (\bI- \bA_S^{-1}) X\ge\tau_k\} \to0$
  and $\bP_S\{X^T (\bI- \bA_S^{-1}) X< \tau_k\} \to0$.
  We use Lemma~\ref{lemqf} and the fact that, by Chebyshev's inequality,
  %
  \[
  \mathbf{P}\bigl\{|\chi_{k}^2 - k| > t_k \sqrt{k}\bigr\} \to0, \qquad
  k \to \infty,
  \]
  %
  for any sequence $t_k \to\infty$, and the fact that
  %
  \[
  \mathbf{P}\{t_k^{-1} < \chi_1^2 < t_k\} \to1  \qquad\mbox{as $k \to
  \infty$}.
  \]
  %
  We choose $t_k = \log k$ and define $\tau_k := -\rho k + \rho t_k
  \sqrt{k} + t_k$.
  Then under the null,
  %
  \[
  \bP_0\{X^T (\bI- \bA_S^{-1}) X \ge\tau_k \} \to0,
  \]
  %
  and under the alternative, setting $\eta_k := -\rho k - \rho t_k \sqrt
  {k} + \rho k t_k^{-1}$,
  %
  \[
  \bP_S\{X^T (\bI- \bA_S^{-1}) X < \eta_k \} \to0.
  \]
  %
  We then conclude with the fact that, for $k$ large enough, $\tau_k <
  \eta_k$.
  
  If $\rho k$ is bounded, the densities of the test statistic under
  both hypotheses have a~significant overlap and the risk cannot
  converge to $0$.
  
  The proof of the second statement is similar.
\end{proof}

\subsubsection{Generalised Moment Method}
When $\cC > 1$, an direct application of the momont method discussed earlier does not yield very promising lowerbounds; instead, we leverage the insight from the Representation Lemma \ref{lemrepresent}.
\begin{prop}[\cite{arias2012correlation}, Theorem 2.1]
  \label{thmlower}
For any class $\cC$ and any $a~> 0$,
\[
R^* \geq\mathbf{P}\{|\cN(0,1)| \le a\} \bigl(1 - \tfrac12 \sqrt{\bE
\exp
(\nu_a~Z ) - 1}\bigr),
\]
where $\nu_a~:= \rho a^2/(1+\rho)- \frac12 \log(1-\rho^2)$ and
$Z=|S \cap S'|$, with $S, S'$ drawn independently, uniformly at random
from $\cC$. In particular, taking $a~= 1$,
\[
R^* \geq0.6 - 0.3 \sqrt{\bE\exp(\nu_1 Z ) - 1},
\]
where $\nu_1 = \nu(\rho) := \rho/(1+\rho)- \frac12 \log(1-\rho^2)$.
\end{prop}

\begin{proof}
  Via Lemma~\ref{lemrepresent}, we can write 
    \[
    X_i = \begin{cases}
    U_i, \quad & \mbox{ if } i \notin S, \cr
    \sqrt{\rho}  U + \sqrt{1-\rho}  U_i, & \mbox{ if } i \in S
    \end{cases}
    \]
  where $U,U_1,\ldots,U_n$ are independent standard normal random variables.
  We consider now the alternative $H_1(u)$, defined as the alternative
$H_1$ given \mbox{$U=u$}.
Let $R(f)$, $L$, $f^*$ [resp., $R_u(f)$, $L_u$, $f_u^*$] be the risk of
a~test $f$, the likelihood ratio, and the optimal (likelihood ratio)
test, for $H_0$ versus $H_1$ [resp., $H_0$~versus $H_1(u)$]. For any $u
\in\bR$, $R_u(f_u^*) \leq R_u(f^*)$, by the optimality of $f_u^*$
for $H_0$ versus $H_1(u)$. Therefore, conditioning on $U$,
$$R^* = R(f^*) = \bE_{U} R_U(f^*) \geq \bE_{U} R_U(f_U^*) = 1 - \tfrac{1}{2} \bE_{U} \bE_0 |L_U(X) - 1|$$
Using the fact that
$\bE_0 |L_u(X) - 1| \le2$ for all $u$, we have
%
\[
\bE_{U} \bE_0 |L_U(X) - 1|
\le2\bP\{|U|>a\} + \bP\{|U|\le a\} \max_{u \in[-a,a]} \bE_0
|L_u(X) - 1|
\]
%
and therefore, using the Cauchy--Schwarz inequality,
%
\begin{eqnarray*}
1 - \frac{1}{2} \bE_{U} \bE_0 |L_U(X) - 1|
& \ge&
\bP\{|U|\le a\}\biggl( 1-\frac{1}{2}\max_{u \in[-a,a]} \bE_0
|L_u(X) - 1|\biggr)
\\
%& \geq& \bP\{|U|\le a\} - \frac12 \max_{u \in[-a,a]} \bE_0 |L_u(X)
%- 1| \\
& \geq& \bP\{|U|\le a\}\biggl(1 - \frac12 \max_{u \in[-a,a]}
\sqrt{\bE_0 L_u^2(X) - 1}\biggr).
\end{eqnarray*}

After some computation, we obtain 
$$\bE_0 L_u^2(X) \le \frac{1}{N^2} \sum_{S, S' \in\cC}
\exp\biggl(\biggl(\frac{\rho u^2}{1+\rho}- \frac12\log(1-\rho
^2)\biggr) |S \cap S'| \biggr)$$
\end{proof}

Again, we reduce the problem to studying the purely combinatorial quantity $Z = |S \cap S'|$. We demonstrate the implications of this proposition via a few examples.
\begin{exmp}[Disjoint Sets, \cite{arias2012correlation}, Section 2.3.1]
  Suppose all $S\in\cC$ are disjoint (and therefore $KN\le n$). Let $Z = K$ with probability $1/N$ and $Z = 0$ otherwise. Thus,
  %
  \[
  \mathbb{E} e^{\nu Z} -1 = \frac{1}{N} (e^{\nu K} -1)
  \le\frac{1}{N} e^{\nu K}
  \]
  %
  which is bounded by $1$ if $\nu\le\log(N)/k$, in which case $R^* \ge0.3$.
\end{exmp}

\begin{exmp}[$k$-intervals, \cite{arias2012correlation}, Section 2.3.2]
  Suppose $\cC$ is the class of all intervals of size $k$ of the form $\{i,\ldots, i + k -1\}$ modulo $n$. Then $N \le n$. For two $k$-intervals chosen independently and uniformly at random,
  \[
  \bP\{|S \cap S'| = \ell\} = \frac2N\qquad\forall\ell= 1,\ldots, k.
  \]
  Thus,
\[
\bE e^{\nu Z}-1 = \frac2N \Biggl(\sum_{\ell=1}^k e^{\nu\ell}
-k\Biggr) \le\frac{2k}N e^{\nu k},
\]
which is bounded by $1$ if $$\nu\le\frac{\log(n/2k)}{k}$$
in which case $R^* \ge0.3$.
\end{exmp}

\begin{exmp}[$k$-sets, \cite{arias2012correlation}, Section 2.3.3]
  Suppose $\cC$ is the class of all sets of size $k$. By negative association, (see Proposition \ref{negass})
  \[
  \bE e^{\nu Z} \le\biggl((e^\nu-1) \frac{k}{n}+1\biggr)^k
  \le\exp\biggl((e^\nu-1) \frac{k^2}{n}\biggr),
  \]
  which is bounded by 2 when 
  $$\frac{k^2}{n} \le\frac{\ln2}{\exp(\nu(\rho)) -1}$$
  in which case $R^* \ge0.3$.
\end{exmp}

\begin{exmp}[Perfect Matchings, \cite{arias2012correlation}, Section 2.3.4]
  Suppose $\cC$ is the class of all perfect matchings of size $k = \sqrt{n}$. Using the same $Z$ as in Example \ref{exmp:Perfect Matchings},
  \[
  \bE e^{\nu Z} \le\biggl((e^\nu-1) \frac{k}{n}+1\biggr)^k
  \le\exp\biggl((e^\nu-1) \frac{k^2}{n}\biggr),
  \]
  which is bounded by 2 when 
  $$\frac{k^2}{n} \le\frac{\ln2}{\exp(\nu(\rho)) -1}$$
  in which case $R^* \ge0.3$.
\end{exmp}

\begin{exmp}[Spanning Trees, \cite{arias2012correlation}, Section 2.3.5]
  Suppose $\cC$ is the class of all spanning trees of a complete graph with $k+1$ vertices. Similar to Example \ref{exmp:Spanning Trees}, notice
  \[
  \bE e^{\nu Z} \le \exp{2(e^\nu-1)},
  \]
  which is bounded by 13/4 when $\nu\le1+\ln((\ln(13/4))/2)$, in which case $R^* \ge0.15$.
\end{exmp}


\subsection{Upper Bounds}
One of the simplest tests is based on the observation that
the magnitude of the squared-sum $(\sum_{i=1}^n X_i)^2$ may be
substantially different under the null and alternative hypotheses due
to the higher correlation under the latter.

Indeed, under $\bP_0$, $(\sum_{i=1}^n X_i)^2$ is distributed as $n
\chi_1^2$, while for
any $S\subset\{1,\ldots,n\}$ with $|S|=k$, under
$\bP_S$, $(\sum_{i=1}^n X_i)^2$ has the same distribution
as $(n + \rho k(k-1)) \chi_1^2$; in fact, under the more general
correlation model~(\ref{model-general}), this is a~(stochastic) lower
bound. This immediately leads to the following result.
%
%pr3.1 #&#
\begin{prop}
\label{prpsq}
Let $\cC$ be an arbitrary class of sets of size $k$
and suppose that $\rho k^2/n \to\infty$ in~(\ref{model-general}).
If $t_n$ is such that $t_n\to\infty$ but $t_n = o(\rho k^2/n)$,
then the test which rejects the null hypothesis
if $(\sum_{i=1}^n X_i)^2 > n t_n$
has a~%GL
worst-case
risk converging to zero.
However, any test based on $(\sum_{i=1}^n X_i)^2$ is powerless if
$\rho k^2/n \to0$ in~(\ref{model}).
\end{prop}

In Corollary~\ref{corksets}, we saw that reliable detection of
$k$-sets is
impossible if $k^2/n \to\infty$ and $\rho k^2/n \to0$. Here we see
that, when $\rho k^2/n \to\infty$, the\vadjust{\goodbreak} squared-sum test is
asymptotically powerful. Hence, the following statement:

\begin{quote}
{\normalsize\textit{The squared-sum test is near-optimal for detecting
$k$-sets in the regime where $k^2/n \to\infty$.}}
\end{quote}
%
On the other hand, in the regime $k^2/n \to0$, the squared-sum test is
powerless even if $\rho= 1$.
The test does not require knowledge of $\rho$, though knowing $\rho$ allows
one to choose the threshold $t_n$ in an optimal fashion; if $\rho$ is
unknown, we simply choose $t_n \to0$ very slowly.

\subsection{The generalized likelihood ratio test}
\label{secglrt}

In this section we investigate the performance of the generalized
likelihood ratio test (GLRT). We show that for parametric classes such
as $k$-intervals, the test is near-optimal. However, for the
nonparametric class of $k$-sets,
the test performs poorly in some regimes.

By definition, the GLRT rejects for large values of $\max_{S \in\cC}
Z_S/\bE_0 Z_S$, or simply $\max_{S \in\cC} Z_S$ when all the sets in
the class $\cC$ are of same size, since~$\bE_0 Z_S$ only depends on
the size of $S$. Hence, the GLRT is of the form
%
\[
f(X)=0  \quad\mbox{if and only if}\quad
\max_{S \in\cC} X^T (\bI- \bA_S^{-1}) X \leq t
\]
%
for some appropriately chosen $t$. We immediately notice that the GLRT
requires knowledge of $\rho$

Our analysis of the GLRT is based on Lemma~\ref{lemqf}, which
provides the
distribution of the quadratic form $X^T (\bI- \bA_S^{-1}) X$ under
the null $\bP_0$ and under the alternative $\bP_S$.
Under the null we need to control the maximum of such quadratic forms
over $S \in\cC$, which we do using exponential concentration
inequalities for chi-squared distributions.

%s3.2.1 #&#
\subsubsection{The GLRT for $k$-intervals and other parametric classes}

Recalling Corollary~\ref{corkint}, when detecting $k$-intervals all
tests are
asymptotically powerless when $\rho\ll\min(1, \log(n/k)/k)$. We
assume for concreteness that
$k/\log n \to\infty$, for otherwise detecting $k$-intervals for very
small $k$ has more to do with detecting $k$-sets. We state a~general
result that applies for classes
of small cardinality.
%
%pr3.2 #&#
\begin{prop} \label{prpglrt-small} Consider a~class $\cC$ of sets of
size $k$, with cardinality $N
\to\infty$ such that $\log(N)/k \to0$. When $\rho k / \log N
\to\infty$, the generalized likelihood ratio test with threshold
value $t = - \rho k + \rho\sqrt{5 k \log N} + 2\log N$ has
%GL Bayes
worst-case
risk tending to zero.
\end{prop}
%
\begin{proof}
We first bound the probability of Type I error. Indeed, under the null,
by Lemma~\ref{lemqf} and its proof, we can decompose
%
\[
X^T (\bI- \bA_S^{-1}) X
= -\frac\rho{1-\rho} C_S + \frac{\rho(k-1)}{1 + \rho(k-1)} D_S,
\]
%
where $C_S \sim\chi_{k-1}^2$ and $D_S \sim\chi_1^2$. Hence,
%
\[
\max_{S \in\cC} X^T (\bI- \bA_S^{-1}) X \leq-\rho\min_{S \in
\cC} C_S + \max_{S \in\cC} D_S.
\]
%
It is well known that the maximum of $N$ standard normals is bounded
by
$\sqrt{2 \log N}$
with probability tending to 1 as $N \to
\infty$. Hence, the second term on the right-hand side is bounded by
$2 \log N$ with high probability. For the first term, we combine the
union bound and Chernoff's bound to obtain, for all $a\le1$,
%
%e3.1 #&#
\begin{eqnarray}\label{glrt-small-eq1}
\bP_0\Bigl\{\min_{S \in\cC} C_S < a~(k-1)\Bigr\}
&\leq& N \mathbf{P}\{\chi_{k-1}^2 < a(k-1)\} \nonumber\\[-8pt]\\[-8pt]
&\leq& N \exp\biggl(-\frac{(k-1)}2 (a~- 1 - \log a)\biggr).
\nonumber
\end{eqnarray}
%
Using the fact that $a~- 1 - \log a~\sim\frac12 (1-a)^2$ when $a~\to
1$, the right-hand side tends to zero when $a~= 1 - \sqrt{(5/k)\log
N}$. We arrive at the conclusion that the GLRT with threshold $t =
- \rho k + \rho\sqrt{5 k \log N} + 2 \log N$ has probability of Type
I error tending to zero.

Now consider the alternative under $\P_S$. By Lemma~\ref{lemqf} and
Chebyshev's inequality,
%
\[
X^T (\bI- \bA_S^{-1}) X \geq- \rho k - \rho s_k \sqrt{k} + \rho k /s_k
\]
%
with high probability when $s_k \to\infty$. We then conclude by the
fact that the right-hand side is larger than $t$ when $s_k \to\infty$
sufficiently slowly.
\end{proof}

Comparing the performance of the GLRT in Proposition~\ref{prpglrt-small} with the
lower bound for $k$-intervals in Corollary~\ref{corkint}, we see that the
GLRT is near-optimal for detecting $k$-intervals. This is actually
the case for all parametric classes we know of.

%s3.2.2 #&#
\subsubsection{The GRLT for $k$-sets and other nonparametric classes}
\label{secglrt-nonparametric}

Consider now the example of the class of all $k$-sets.
Compared to the previous section, the situation here is different in that
$N$, the size of the class $\cC$, is much larger. For example, for
$k$-sets, $N = {n \choose k}$, and
therefore $\log(N)/k \to\infty$ with $n \to\infty$. The
equivalent\vspace*{1pt} of Proposition~\ref{prpglrt-small} for this regime is the
following:
%
%pr3.3 #&#
\begin{prop} \label{prpglrt-large}
Consider a~class $\cC$ of sets of size $k$, with cardinality $N \to
\infty$ such that $\log(N)/k
\to\infty$. When $\eta:= (1-\rho) N^{2/k} (\log N)/k \to0$, the
generalized likelihood
ratio test with threshold value $t = -(\log N)/\sqrt{\eta}$ has
%GL Bayes
worst-case
risk tending to zero.
\end{prop}
%
\begin{proof}
We follow the proof of Proposition~\ref{prpglrt-small}. The only
difference is in~(\ref{glrt-small-eq1}), where we now need $a~\to0$ and that
right-hand side tends to zero when $\log a~+ 2 (\log N)/k \to-\infty
$. Choose $a~= N^{-2/k} \sqrt{\eta}$, obtaining that, with\vadjust{\goodbreak} high probability,
%
%e3.2 #&#
\begin{equation} \label{glrt-large-eq1}
\max_{S \in\cC} X^T (\bI- \bA_S^{-1}) X \leq-\frac\rho{1-\rho}
N^{-2/k} k \sqrt{\eta} + 2 \log N.
\end{equation}
%
As before, with high probability under $\P_S$,
%
%e3.3 #&#
\begin{equation} \label{glrt-large-eq2}
X^T (\bI- \bA_S^{-1}) X \geq- \rho k,
\end{equation}
%
so we only need to check that the threshold $t$ is larger than the
right-hand side in~(\ref{glrt-large-eq1}) and smaller than the
right-hand side in~(\ref{glrt-large-eq2}), which is the case by the
assumptions we made.
\end{proof}

Notice that in Proposition~\ref{prpglrt-large} the condition on $\rho
$ implies
that $\rho\to1$, which is much stronger than what the squared-sum
test requires when $k^2/n \to\infty$.
For $k$-sets, $N = {n \choose k}$---so that $\log N = k \log(n/k) +
O(k)$---and the requirement is that $(1-\rho) (n/k)^2 \log(n/k) \to
0$, which is substantially stronger than what the lower bound obtained
in Corollary~\ref{corksets} requires. Moreover, if we restrict $\rho
$ to be bounded away from $1$, then the GLRT may be powerless.
%
%
%th3.1 #&#
\begin{thm}
\label{thmglrt-bad}
Let $\cC$ be the class of all $k$-sets. If $\rho< 0.6$ and $k=
o(n^{0.7})$, the GLRT has a~%GL
Bayes
risk bounded away from zero.
\end{thm}

In view of Theorem~\ref{thmglrt-bad}, the GLRT is clearly suboptimal
when in
the situation stated there, and compares very poorly with the
squared-sum test, which is asymptotically powerful if $\rho k^2/n \to
\infty$ as seen in Proposition~\ref{prpsq}. We do not know of any
other situation
where the GLRT fails so miserably.

%s3.3 #&#
\subsection{A~localized squared-sum test}

While the GLRT is near-optimal for detecting objects from a~parametric
class such as $k$-intervals, it needs knowledge of $\rho$.
However, a~simple modification solves this drawback. Indeed, consider
the following ``local'' squared-sum test:
%
\[
f(X)=0  \quad\mbox{if and only if}\quad
\max_{S \in\cC} \biggl(\sum_{i \in S} X_i\biggr)^2 \leq t
\]
%
for some appropriate threshold $t$.
%
%pr3.4 #&#
\begin{prop} \label{prplocal-sum}
Consider a~class $\cC$ of sets of size $k$, with cardinality $N
\to\infty$ such that $\log(N)/k \to0$. When $\rho\gg\log(N)/k$
in~(\ref{model-general}), the local squared-sum test with threshold $t
= 2 k \log N$ has
%GL
worst-case
risk tending to zero.
\end{prop}
%
\begin{proof}
The proof is quite straightforward. Indeed, under the null, for any $S$
of size $k$ we have $\sum_{i \in S} X_i \sim\cN(0, k)$ so that
%
\[
\max_{S \in\cC} \biggl(\sum_{i \in S} X_i\biggr)^2 \leq t\vadjust{\goodbreak}
\]
%
with probability tending to 1. Under an alternative~(\ref{model-general}), $S$ denoting the anomalous set of variables, we have
%
\[
\P\biggl( \biggl(\sum_{i \in S} X_i\biggr)^2 \ge t\biggr) \ge\P\bigl( \bigl(k + k(k-1) \rho
\bigr) \chi_1^2 \ge t\bigr) \to1,
\]
%
when $\rho\gg\log(N)/k$.
\end{proof}

Specializing this result to the case of $k$-intervals leads to the
following statement (which ignores logarithmic factors):

\begin{quote}
{\normalsize\textit{The localized squared-sum test is near-optimal for
detecting $k$-intervals in the regime where $\log(n)/k \to0$.}}
\end{quote}

\textit{When $k$ is unknown.} We might only know that some interval is
anomalous, without knowing the size of that interval. In that case,
multiple testing at each $k$ using the local squared-sum test yields
adaptivity. Computationally, this may be done effectively by computing
sums in a~multiscale fashion as advocated in~\cite{MGD}. In fact, here
it is enough to compute the sums over all \textit{dyadic}
intervals---since each interval $S$ contains a~dyadic interval of
length at least $|S|/4$---and this can be done in $3 n$ flops in a~recursive fashion.

%s3.4 #&#
\subsection{A~goodness-of-fit test}

By now, the parametric case is essentially solved, with the local
squared-sum test being not only near-optimal but also computable in
polynomial time
%GL
(in $n$ and $k$)
for the case of $k$-intervals, for example. In the nonparametric case,
so far, the story is not complete. We focus on the class of all
$k$-sets. There we know that the squared-sum test is near-optimal if
$k^2/n \to\infty$. If $k^2/n \to0$, it has no power, and we only
know that the GLRT works when $(1-\rho) (n/k)^2 \log(n/k)\to0$,
which does not match the rate obtained in Corollary~\ref{corksets}.
Worse than that, it is not clear whether computing the GLRT is possible
in time polynomial in $(n, k)$.
We now show that a~simple
goodness-of-fit (GOF)
test performs (almost) as desired.

%GL sentence added
The basic idea is the following.
Let $H_i = \Phi^{-1}(X_i)$, where $\Phi$ is the standard normal distribution
function. Under the null, the $H_i$'s are i.i.d. uniform in $(0,1)$.
Under an alternative with anomalous set denoted by $S$, the $X_i, i
\in S$ are closer together, especially since we place ourselves in the
regime where $\rho\to1$. More precisely, we have the following.
%
%le3.1 #&#
\begin{lem} \label{lemclose}
Suppose $X_1,\ldots, X_k$ are
%GL standardized
zero-mean, unit-variance
random variables satisfying $\Cov(X_i, X_j) \geq\rho> 0$, for all $i
\neq j$. Let $\overline{X}$ denote their average. Then for any $t > 0$,
%
\[
\bP\bigl\{ \# \{i: |X_i - \overline{X}| > t\} \geq k/2\bigr\}
\leq\frac{2(1-\rho)}{t^2}.
\]
%
\end{lem}
%
\begin{proof}
Let $\Lambda:= \sum_{i \neq j} \Cov(X_i, X_j) \geq k(k-1) \rho$.
Elementary calculations show that
%
\[
\bE\biggl[\frac1k \sum_i (X_i - \overline{X})^2\biggr]
= 1 -\frac1k -\frac{\Lambda}{k^2} \leq(1 -1/k)(1 - \rho) \leq
1-\rho.
\]
%
By Markov's inequality, we then have
%
\[
\bP\biggl\{ \frac1k \sum_i (X_i - \overline{X})^2 > t^2/2
\biggr\}
\leq\frac{2(1-\rho)}{t^2}.
\]
%
The statement follows from observing that
%
\[
\# \{i: |X_i - \overline{X}| > t\} \geq k/2  \quad\Rightarrow\quad
\frac1k \sum_i (X_i - \overline{X})^2 > t^2/2.
\]
%
\end{proof}

The idea, therefore, is detecting unusually high concentrations of
$H_i$'s, which is a~form of GOF test for the uniform distribution.
Under a~general correlation model as in~(\ref{model-general}), with
Lemma~\ref{lemclose} we see that the concentration will happen over an
interval of length slightly larger than $\sqrt{1-\rho}$. This is
apparent from Lemma~\ref{lemrepresent} under the simple correlation model
(\ref{model}).

Choose an integer $m$ such that $m \gg(n/k^2) \log(n/k^2)$ and partition
the interval $[0,1]$ into $m$ bins of length $1/m$, denoted $I_s,  s =
1,\ldots, m$. Let $B_s = \# \{i: H_i \in I_s\}$ be the bin
counts---thus, we are computing a~histogram. Then consider the
following GOF test:
%
\[
f(X)=0  \quad\mbox{if and only if}\quad
\max_{s = 1,\ldots, m} B_s \leq t,
\]
%
where $t$ is some threshold.
%
%Assume first that $\rho$ is known.
%
%pr3.5 #&#
\begin{prop} \label{prpgof}
Consider the class $\cC$ of all $k$-sets in the case where $k^2/n \to
0$ and $k/\log n \to\infty$. In the GOF test above, choose $m$ such
that $(n/k^2) \log n \ll m \ll n/\log n$. When $(1-\rho)^{1/2} \ll
1/m$ in~(\ref{model-general}), the resulting test with threshold $t =
n/m + \sqrt{3 n \log(m)/m}$ has
%GL
worst-case
risk tending to zero.
\end{prop}
%
\begin{proof}
Bernstein's inequality, applied to the binomial distribution, gives that
%
\[
\bP_0\bigl\{B_s > n/m + b \sqrt{n/m}\bigr\} \leq\exp\bigl[- (b^2/2)/\bigl(1 + (b/3)
\sqrt{m/n}\bigr)\bigr].
\]
%
This and the union bound imply that, indeed,
%
\[
\bP_0\Bigl\{\max_s B_s > t\Bigr\} \to0.
\]

Consider now an alternative of the form~(\ref{model-general}), with
$S$ denoting the anomalous set. Let
%
\[
I := \{i \in S: |X_i - \overline{X}_S| \leq1/m\},\qquad  \overline
{X}_S := \frac1k \sum_{i \in S} X_i.\vadjust{\goodbreak}
\]
%
Though the set $I$ is random, by Lemma~\ref{lemclose} and the fact that
$(1-\rho)^{1/2} \ll1/m$, we have that
%
\[
\bP_S\{|I| \geq k/2\} \to1.
\]
%
Define the event $Q := \{-a~\leq\overline{X}_S \leq a\}$ for some $a~> 0$. Note that, since the variance of $\overline{X}_S$ is bounded by
1, $\bP(Q^c) \leq2 (1 - \Phi(a))$. Define $\tilde{H}_S = \Phi
^{-1}(\overline{X}_S)$. On~$Q$, using a~simple Taylor expansion, we have
%
\[
|H_i - \tilde{H}_S| \leq\frac{|X_i - \overline{X}_S|}{\phi(a~+
1/m)} \leq e^{a^2}/m\qquad \forall i \in I,
\]
%
where $\phi$ denotes the standard normal density function and $a$
is taken sufficiently large. Therefore, when $|I| \geq k/2$ and $Q$
hold, at least $k/2$ of the anomalous $H_i$'s fall in an interval of
length at most $2 e^{a^2}/m$. Since such an interval is covered by at
most $2 e^{a^2}$ bins, by the pigeonhole principle, there is a~bin
that contains $k e^{-a^2}/4$ anomalous $H_i$'s. By Bernstein's
inequality, the same bin will also contain at least $(n-k)/m -\sqrt{3
n \log(m)/m}$ nonanomalous $H_i$'s (with\vspace*{1pt} high probability), so in
total this bin will contain $n/m -k/m -\sqrt{3 n \log(m)/m} + k
e^{-a^2}/4$ points. By our choice of $m$, $k \gg\sqrt{n \log(m)/m}$,
so it suffices to choose $a~\to\infty$ slowly enough that $k e^{-a^2}
\gg\sqrt{n \log(m)/m}$ still. Then, with high probability, there is
a~bin with more than $t$ points.
\end{proof}

Ignoring logarithmic factors, we are now able to state the following:

\begin{quote}
{\normalsize\textit{The GOF test is near-optimal for detecting $k$-sets in
the regime where $k^2/n \to0$ and $k/\log n \to\infty$.}}
\end{quote}

When $k/\log n \to0$, things are somewhat different. There, the GOF
test requires that $(1-\rho) n^{2 k/(k-1)} \to0$, which is still
close to optimal when $k \to\infty$, but far from optimal when $k$ is
bounded (e.g., when $k=2$, the exponent is 4 instead of~2).
Indeed, when $k/\log n \to0$, $m$ needs to be chosen larger than $n$,
and Bernstein's inequality is not accurate. Instead, we use the simple bound
%
\[
\P\bigl(\operatorname{Bin}(n,p) \ge\ell\bigr) \le2 \frac{ (n p)^\ell}{\ell!}
\qquad\mbox{when } n p \le1/2.
\]
%
Note that Bennett's inequality would also do. (The analysis also
requires some refinement showing that, with probability tending to 1
under the alternative, one cell contains at least $k$ points.)
Note that in the remaining case, $k = O(1)$, the GLRT is optimal up to
a~logarithmic factor, since it only requires that $(1-\rho) n^2 \log n
\to0$, as seen in Section~\ref{secglrt-nonparametric}. We do not
know whether
a~comparable performance can be achieved by a~test that does not have
access to $\rho$.

\textit{When $k$ is unknown.} In essence, we are trying to detect an
interval with a~higher mean in a~Poisson count setting.\vadjust{\goodbreak} As before, it
is enough to look at dyadic intervals of all sizes, which can be done
efficiently as explained earlier, following the multiscale ideas
in~\cite{MGD}.

\section{Extension}

\bibliographystyle{plain}
\bibliography{bibfile}
\end{document}
